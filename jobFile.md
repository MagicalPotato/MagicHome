##### 带领团队完成精准派单需求的设计和开发:
* 需求背景:通讯基站经常会因为各种原因而产生一些告警,当告警产生后,有的会自动恢复,有的则需要人工手动修复.如果属于人工修复的告警,则需通知就近
的维修站派维修员实地检修.但是不同的告警检修可能需要不同的工具和材料,如果事先不清楚具体告警,则会导致维修员因为维修材料准备不足或者工具带不全而多次往返,造成大量的时间和人力浪费,所以在通知维修员前去检修之前要尽可能定位出告警的详细信息.精准派单项目就是应这种要求而生,目的就是通过分析采集的数据找到告警发生的根本原因,从而准确判断告警类型,做到高效快速地修复故障,节省成本和保持通信畅通.
* 整体流程: 基站工作过程中产生的工作数据和日志会定时上传到中央服务器中,当告警发生后,会发消息到本地的控制器,也就是本项目的控制器.控制器收到消息后会根据告警信息通过ftp将服务器上与当前告警相关的配置,监测,话务统计等数据采集到本地,接着对原始数据进行进一步的清洗,处理成合适的结构并存入mysql数据库,入库完成之后启动分析微服务,微服务根据告警诊断算法去取相关数据进行分析,完成后将结果汇总并入库同时推送到中央控制器,中央控制器收到消息后根据处理的结果执行后续的任务派发等工作.
* 项目架构: 项目采用微服务架构.task微服务相当于控制器,主管任务分发,进程调度.当收到ftp采集完成的消息之后,task通知dataClean微服务进行数据清洗,数据清洗完成之后会启动各个数据导入的微服务进行数据导入.数据导入的微服务有多个,对应原始数据的类型,导入时并行的,每个微服务只管自己需要导入的数据,当所有数据都导入完成之后task通知subjectAnalysis微服务去分析数据生成结果并入库同时向中央控制器推送.不同的微服务之间通信通过扫描指定的任务表来完成,每个微服务会定时去扫描任务表中有没有自己需要完成的任务,如果有就进行,没有则继续扫描.直到领到自己需要做的任务.
* 技术点: 1. 告警,监测,话务统计等数据的格式不统一,且会由于各种原因导致单条数据也不完整,所以数据清洗这块是个比较大的麻烦,最终是数据的使用场景将原始数据分成了四大类,每个类的存储结构有所不同,但是都统一转成json格式入库,使用时也根据对应种类进行解析. 2.整个分析流程要控制在20分钟之内,为了节省时间采集数据使用了多线程,每个线程下载总数据的一部分  3. task调度各个微服务和微服务之间的通信也是个麻烦,如果使用消息队列则会增加额外的工作量且难度也会增加不少,但项目预算时间有限所以最终是通过更新数据表和微服务扫描数据表来解决的.  
* 项目职责: 第二负责人 : 参与项目整体流程的设计,需求划分,工作量评估,风险评估,人力安排等.参与微服务调度流程逻辑设计,完成数据清洗核心算法的编写和优化.
##### 带领团队完成lite商用 自动化部署需求的设计和开发
* 需求背景: lite的需求涉及到环境的部署,以往在新环境上部署lite都是人力手动去拷贝相应的自动化包和需要的各种文件,拷贝完成之后再手动执行自动化部署包中
的不同脚本文件,以实现对整个环境全新安装或者升级部署.但是人工操作有时候容易出错,且在需要部署大量机器的集群环境时会耗费大量的时间和精力,远不能满足高效快速的软件要求.基于这种考虑,该自动化部署环境的需求便应运而生.以期能够通过简单的点击页面,便能够实现单台或多台机器的文件上传和部署.
* 整体流程: 首先在目标环境上拷贝一份自动化部署的微服务并启动,这个微服务就是控制所有环境部署的控制中心.通过浏览器输入部署地址进入到部署页面,根据页面提示填写待部署的环境信息,然后启动部署,微服务便会自动去完成拷贝和启动各种脚本文件的操作.启动后会跳转日志页面,可以根据需求选择展示哪个环境的部署日志信息.等到所有环境都部署完成之后,会有验证环境的基本功能是否正常的环节,通常包括kafka,ftp,hbase等功能的验证.如果验证都没有问题,则说明当前环境已经部署完成.
* 项目架构 : 当前项目也是采用微服务架构,但是最终用到的微服务实际上只有一个,所以相当于是个单体流程.与部署有关的控制流程以及部署完成之后的验证都在这个微服务中完成.只要微服务正常启动,其他操作按照页面提示进行即可.
* 技术点 : 1.由于每个环境所部署的微服务可能不止一个,且环境有时候会自动结束一些闲置的java微服务的进程,所以这个自动化微服务不能使用环境原有的java环境,需要自带java环境以避免自身的进程被环境误杀. 2.大批量的部署涉及到跨虚拟机的数据拷贝和脚本调用,对相关doc命令的熟悉程度,以及java侧对各种命令的调用需要仔细对待. 
* 职责: 项目第一负责人, 负责设计项目的整体流程,需求划分,工作安排,进度把控等. 完成java侧doc命令以及调用doc命令代码的编写.
##### python爬虫
* 独立构思和完成轻量级python爬虫的编写.能够对实现对诸如网易新闻,时光网等网站信息的爬取.虽然能够实现功能,但是整体上功能还是比较局限,抗性不足,后续想使用框架将其改进成可单独部署的分布式大批量的爬虫,用数据库存储爬取的数据,并能够对爬取的数据进行过滤和校验,以及一些逻辑上的优化和异常处理. 爬虫能不能爬取到需要的数据,一方面是爬虫本身,另一方面主要是对网站的分析,想要完成一个优秀的爬虫不但要对后台熟练,还得熟悉前台,html以及各种协议等.这对一个主要从事后台的开发来说还是个不小的挑战.
